---
title: "Qualifying Weight Lifting Workout"
author: "Luca Lo Iacono"
date: "5/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


require(dplyr); require(ggplot2); require(GGally); require(caret); require(splines)
require(formattable); require(gtools); require(compiler); require(parallel);
require(rlist); require(combinations); require(rattle); require(mclust);
require(parallel); require(doParallel)

computeOOBErrEst <- function(x) {
  cm <- x$confusion
  cm <- cm[, -ncol(cm)]
  1 - sum(diag(cm)) / sum(cm)
}

# Number of copies to be run on localhost when paralleling calculations
copies <- 3
set.seed(32343)

```

## Executive Summary
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The goal of the project is to find a classifier that predicts how well the exercise is performed, using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants as predictors. 
We tried to model different classifiers. We found three models that are supposed to have excellent accuracy in predicting how well the exercise is performed.


### Acknowledgements
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

## Exploratory Data Analysis
Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways, in particular one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. It was made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz6tnHbqlcO

Our goal is to predict the manner in which participants did the exercise. This is the "classe" variable in the training set (pml-training.csv).
A value for "classe" is not given in pml-testing.csv, for which we have to try a prediciton after having build an effective model.

```{r, , echo = FALSE}
# In sample data set ####
insample.dataset = read.csv("~/Documents/Oppimi/RnD/DG Console/Data/R/PracticalMachineLearningClass/pml-training.csv")

# Removing useless features
useless.features <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "new_window", "num_window", "cvtd_timestamp")
selected.features <- names(insample.dataset)[!names(insample.dataset) %in% useless.features]

insample.dataset <- insample.dataset[, selected.features]

# Reformat cvtd_timestamp as Date
# insample.dataset$cvtd_timestamp <- as.factor(as.Date(insample.dataset$cvtd_timestamp, tryFormats = c("%d/%m/%Y")))

# Check which features have the same NAs as max_roll_belt (max_roll_belt is just one of the first features with many NAs)
na.rows <- which(is.na(insample.dataset$max_roll_belt))

# Features with many NAs
features.with.same.nas <- sapply(insample.dataset, function(x) identical(which(is.na(x)), na.rows))
# names(insample.dataset)[features.with.same.nas]

insample.dataset.na.omit.cols <- insample.dataset[, -which(features.with.same.nas)]

# Remove features with many NAs
train.index <- caret::createDataPartition(insample.dataset.na.omit.cols$classe, p = 0.7, list = FALSE)
training <- insample.dataset.na.omit.cols[train.index, ]
testing <- insample.dataset.na.omit.cols[-train.index, ]

# Near zero features
nvs <- caret::nearZeroVar(training, saveMetrics = TRUE)
near.zero.features <- row.names(nvs[nvs$nzv,])

# Remove near.zero.features
training <- training[-which(names(training) %in% near.zero.features)]
testing <- testing[-which(names(testing) %in% near.zero.features)]


# Out of Sample data set (Course Quiz) ####
outofsample.dataset = read.csv("~/Documents/Oppimi/RnD/DG Console/Data/R/PracticalMachineLearningClass/pml-testing.csv")

# Remove useless features
outofsample.dataset <- outofsample.dataset[, selected.features[-which(selected.features == "classe")]]

# Reformat cvtd_timestamp as Date
# outofsample.dataset$cvtd_timestamp <- as.factor(as.Date(outofsample.dataset$cvtd_timestamp, tryFormats = c("%d/%m/%Y")))

# Remove near.zero.features
outofsample <- outofsample.dataset[-which(names(outofsample.dataset) %in% near.zero.features)]


```

The *in sample* data set (pml-training.csv) includes *`r dim(insample.dataset)[1]` observations* of **`r dim(insample.dataset)[2] -1` variables*.

After analyzing the structure of the data set we noticed that some features might be useless.
Since we are trying to model a classifier that predicts the manner in which exercise are done, the model should be independent from the participants and from workout time.
Therefore we will not include `r paste0(useless.features, collapse = ", ")` in our model.

Besides we are not going to include variables with too many NAs, such as `r paste0(names(insample.dataset)[features.with.same.nas][1:3], collapse = ", ")`, etc., which have `r length(na.rows)` NAs out of `r dim(insample.dataset)[1]` observations.

And we are not going to include near zero variance predictors, such as `r paste0(useless.features, collapse = ", ")`.

We think it might be interesting to consider the different workout session as a factor because something different might be happended in each of them.


### Data Partition

We split the *in sample* data set in two data sets, which we are going to use to model (training data set) and to test (testing data set) each classifier.
70% of the *in sample* data set is dedicated to training data set, 30% to testing data set.
In particular training data set includes `r dim(training)[1]` observations and testing data set includes `r dim(testing)[1]` observations.


## Classifier selection

Since we are trying to build a multiclass classifier using many different predictors we should focus our attention to Trees, which work well in non linear settings.

We should try three different models and check which performs better on the given data set:

 - Bootstrap Aggregating Model
 - Ramdom Forest Model
 - Bosting with Tree Model

We use default bootstrap resampling for all fittings.


### Bootstrap Aggregating Model (TREEBAG)

```{r, , echo = FALSE}
cl <- parallel::makePSOCKcluster(copies)
doParallel::registerDoParallel(cl)

model.treebag <- caret::train(classe ~.,
                              data = training,
                              method = "treebag",
                              # na.action = na.omit,
                              # preProcess = c("center", "scale"),
                              # trControl = trainControl(method = "cv"),
                              verbose = FALSE)

model.treebag

pred.treebag <- predict(model.treebag, testing)

result.treebag <- predict(model.treebag, outofsample)

parallel::stopCluster(cl)

```

#### Confusion Matrix

```{r, , echo = FALSE}
cm.treebag <- caret::confusionMatrix(pred.treebag, as.factor(testing$classe))
cm.treebag
accuracy.treebag <- cm.treebag$overall["Accuracy"]

```

### Ramdom Forest Model (RF)

```{r, , echo = FALSE}
cl <- parallel::makePSOCKcluster(copies)
doParallel::registerDoParallel(cl)

model.rf <- caret::train(classe ~.,
                         data = training,
                         method = "rf",
                         # na.action = na.omit,
                         # preProcess = c("center", "scale"),
                         # trControl = trainControl(method = "cv"),
                         verbose = FALSE)

model.rf

pred.rf <- predict(model.rf, testing)

result.rf <- predict(model.rf, outofsample)

parallel::stopCluster(cl)

```

#### Confusion Matrix

```{r, , echo = FALSE}
cm.rf <- caret::confusionMatrix(pred.rf, as.factor(testing$classe))
cm.rf
accuracy.rf <- cm.rf$overall["Accuracy"]

```

### Boosting with Tree Model (GBM)

```{r, , echo = FALSE}
cl <- parallel::makePSOCKcluster(copies)
doParallel::registerDoParallel(cl)

model.gbm <- caret::train(classe ~.,
                          data = training,
                          method = "gbm", 
                          # na.action = na.omit,
                          # preProcess = c("center", "scale"),
                          # cv.folds = 10,
                          verbose = FALSE)

model.gbm

pred.gbm <- predict(model.gbm, testing)

result.gbm <- predict(model.gbm, outofsample)

parallel::stopCluster(cl)

```


#### Confusion Matrix

```{r, , echo = FALSE}
cm.gbm <- caret::confusionMatrix(pred.gbm, as.factor(testing$classe))
cm.gbm
accuracy.gbm <- cm.gbm$overall["Accuracy"]

```


## Classifiers Comparison

```{r, , echo = FALSE}
min.accuracy <- floor(100*min(c(accuracy.treebag, accuracy.rf, accuracy.gbm)))
msg <- ""
msg.conclusion <- ""

if (min.accuracy > 90) {
  msg <- paste0("All the models have an *excellent* accuracy which is greater than ",
                min.accuracy,
                "%.")
  msg.conclusion <- "Sensitivity and specificity are very good, and these give similar results in predicting out of sample values of *classe*."
} else if (min.accuracy > 80) {
  msg <- paste0("All the models have an *good* accuracy which is greater than ",
                min.accuracy,
                "%.")
  msg.conclusion <- "Sensitivity and specificity are good, and these give similar results in predicting out of sample values of *classe*."
} else if (min.accuracy > 70) {
  msg <- paste0("All the models have an *acceptable* accuracy which is greater than ",
                min.accuracy,
                "%.")
} else {
  msg <- paste0("All the models have an *poor* accuracy which is greater than ",
                min.accuracy,
                "%.")
}

accuracy.sd <- sd(c(accuracy.treebag, accuracy.rf, accuracy.gbm))
msg.sd <- ""
if (accuracy.sd < 0.05) {
  msg.sd <- paste0("All the models have a *very similar* accuracy, which means that the three models have similar performances.")
} else if (accuracy.sd < 0.1) {
  msg.sd <- paste0("All the models have a *similar* accuracy, which means that the three models have similar performances.")
    msg.conclusion <- "Sensitivity and specificity are similar, and these give similar results in predicting out of sample values of *classe*."
} else if (accuracy.sd < 0.5) {
  msg.sd <- paste0("The models have *not similar* accuracy, which means that the three models have not similar performances.")
    msg.conclusion <- "Sensitivity and specificity are different, and we need majority voting to predict out of sample values of *classe*."
} else {
  msg.sd <- paste0("The models have quite different performance in terms of accuracy.")
  msg.conclusion <- "Sensitivity and specificity are very different, and we need majority voting to predict out of sample values of *classe*."
}
```

We just fitted three different models:

 - Bootstrap Aggregating Model with an accuracy of `r floor(100*cm.treebag$overall["Accuracy"])`%
 - Ramdom Forest Model with an accuracy of `r floor(100*cm.rf$overall["Accuracy"])`%
 - Bosting with Tree Model with an accuracy of `r floor(100*cm.gbm$overall["Accuracy"])`%

`r msg`

`r msg.sd`

Here we can see that the prediction of the three models on the "out of sample" data.
We added a column with a majority vote result to highlight the differences.


```{r, , echo = FALSE}
combined.results <- data.frame(result.treebag, result.rf, result.gbm)

majority.vote <- sapply(as.data.frame(t(combined.results)),
                             function(x) mclust::majorityVote(x)$majority)

data.frame(combined.results, majority.vote)

```


## Conclusion
We found three different classifier with an accuracy greater than `r min.accuracy`.

`r msg.sd`

These models proved to predict correctly all the 20 *classe* values from *Out of Sample* data set.



